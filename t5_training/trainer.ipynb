{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42727f8f7eb445a583749b50c206b961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from dataset_utils import T5Dataset, collator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "model_variants = [\"google-t5/t5-base\", \"google-t5/t5-3b\", \"google/flan-t5-large\", \"google-t5/t5-11b\"]\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_variants[0],\n",
    "    torch_dtype=torch.float16,\n",
    "    token = os.getenv('HF_ACCESS_TOKEN')\n",
    ").to(device)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    model_variants[0]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.utils\n",
    "import numpy as np\n",
    "\n",
    "class T5Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts,\n",
    "        tokenizer,\n",
    "        max_length=512,\n",
    "        corruption_rate=0.15,\n",
    "        mean_span_length=3\n",
    "    ):\n",
    "        # super().__init__()\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.corruption_rate = corruption_rate\n",
    "        self.mean_span_length = mean_span_length\n",
    "\n",
    "    def corrupt_text(self, tokens):\n",
    "        # tokens = text.strip().split()\n",
    "        mask = self.generate_span_mask(len(tokens))\n",
    "        input_tokens = []\n",
    "        target_tokens = []\n",
    "        sentinel = 0\n",
    "        in_span = False\n",
    "        for i, token in enumerate(tokens):\n",
    "            if mask[i]:\n",
    "                if not in_span:\n",
    "                    input_tokens.append(f\"<extra_id_{sentinel}>\")\n",
    "                    target_tokens.append(f\"<extra_id_{sentinel}>\")\n",
    "                    sentinel += 1\n",
    "                    in_span = True\n",
    "                target_tokens.append(token)\n",
    "            else:\n",
    "                input_tokens.append(token)\n",
    "                in_span = False\n",
    "\n",
    "        if in_span:\n",
    "            target_tokens.append(f\"<extra_id_{sentinel+1}>\")\n",
    "        else:\n",
    "            input_tokens.append(f\"<extra_id_{sentinel}>\")\n",
    "            target_tokens.append(f\"<extra_id_{sentinel}>\")\n",
    "        \n",
    "        return \" \".join(input_tokens), \" \".join(target_tokens)\n",
    "\n",
    "    def generate_span_mask(self, seq_len):\n",
    "        num_tokens_to_mask = max(1, int(self.corruption_rate * seq_len))\n",
    "        mask = np.zeros(seq_len, dtype=bool)\n",
    "        num_masked = 0\n",
    "        while num_masked < num_tokens_to_mask:\n",
    "            span_start = np.random.randint(0, seq_len)\n",
    "            span_length = max(1, np.random.poisson(self.mean_span_length))\n",
    "            span_end = min(seq_len, span_start + span_length)\n",
    "            if np.any(mask[span_start:span_end]):\n",
    "                continue\n",
    "            mask[span_start:span_end] = True\n",
    "            num_masked += span_end - span_start\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = text.strip().split()\n",
    "        \n",
    "        # Handle empty text case first\n",
    "        if len(tokens) == 0:\n",
    "            input_ids = self.tokenizer.encode(\"\", return_tensors=\"pt\").squeeze(0)\n",
    "            target_ids = self.tokenizer.encode(\"\", return_tensors=\"pt\").squeeze(0)\n",
    "            return {\"input_ids\": input_ids, \"labels\": target_ids}\n",
    "        \n",
    "        try:\n",
    "            corrupted_input, target = self.corrupt_text(tokens)\n",
    "            input_ids = self.tokenizer.encode(\n",
    "                corrupted_input, \n",
    "                truncation=True, \n",
    "                max_length=self.max_length, \n",
    "                return_tensors=\"pt\"\n",
    "            ).squeeze(0)\n",
    "            \n",
    "            target_ids = self.tokenizer.encode(\n",
    "                target, \n",
    "                truncation=True, \n",
    "                max_length=self.max_length, \n",
    "                return_tensors=\"pt\"\n",
    "            ).squeeze(0)\n",
    "            \n",
    "            return {\"input_ids\": input_ids, \"labels\": target_ids}\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {idx}: {e}\")\n",
    "            # Fallback to empty tokens\n",
    "            input_ids = self.tokenizer.encode(\"\", return_tensors=\"pt\").squeeze(0)\n",
    "            target_ids = self.tokenizer.encode(\"\", return_tensors=\"pt\").squeeze(0)\n",
    "            return {\"input_ids\": input_ids, \"labels\": target_ids}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "\n",
    "def collator(batch, tokenizer):\n",
    "    try:\n",
    "        input_ids = [item['input_ids'] for item in batch]\n",
    "        labels = [item['labels'] for item in batch]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    except:\n",
    "        print(batch)\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": input_ids.ne(tokenizer.pad_token_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collator(batch):\n",
    "    return collator(batch, tokenizer)\n",
    "\n",
    "raw_input = '/home/tadesa1/ADBMO-UNLV/data/processed_output_raw.txt'\n",
    "with open(raw_input, 'r') as f:\n",
    "    text = f.readlines()\n",
    "    text = [line for line in text if line.strip()]\n",
    "\n",
    "dataset = T5Dataset(text, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=my_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].shape)       # (batch_size, seq_len)\n",
    "    print(batch[\"labels\"].shape)\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"labels\"])\n",
    "    print(\"-----------------\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_t5_unsupervised(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer=None,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    num_epochs=3,\n",
    "    accumulation_steps=1,\n",
    "    save_path=\"t5_finetuned.pt\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for step, batch in enumerate(loop):\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer step\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f\"{save_path}_epoch{epoch+1}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t5_unsupervised(model, dataloader, collator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
