{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5STo30E4C57o"
   },
   "source": [
    "# Paper Filtering Criterias\n",
    "1. Paper must be original research article and (not a review, poster or preprint) ---**Programatically Solvable**---\n",
    "  - Include Publication Type Field in Search Query\n",
    "\n",
    "2. Paper must have an AD focus.This means: ---**Optimized Search Query + LLM Task**---\n",
    "  - Either having a population of AD patients or\n",
    "  - Looking at Alzheimer disease specific biomarkers  \n",
    "\n",
    "\n",
    "3. Human sample size must be over 50 ---**LLM Task**---\n",
    "  - Extract *Abstract* and *Methods* sections from calls to BioC API\n",
    "  - Feed extracted data into LLM that has been prompt engineered to search and filter papers based on the desired criterias.\n",
    "\n",
    "\n",
    "4. Must be looking at a protein. ---**Optimized Search Query + LLM Task**---\n",
    "  - Amyloid β\n",
    "  - Tau\n",
    "  - Amyloid Precursor Protein\n",
    "  - Presenilin-1 and Presenilin-2\n",
    "  - Apolipoprotein E (ApoE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. Fluid samples like CSF, blood from animal models? ---**Optimized Search Query + LLM Task**---\n",
    "  - How is this different from clinical models?\n",
    "\n",
    "\n",
    "6. \"Blood\" vs \"Blood Pressure\" ---**Optimized Search Query + LLM Task**---\n",
    "  - How is blood being used in the paper? Is the paper using blood pressure as a biomarker or are actual blood samples being taken for biomarkers.\n",
    "\n",
    "\n",
    "7. Papers from 2024 and onward. ---**Optimized Search Query**---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXgAMtiTRBrd"
   },
   "source": [
    "## LLM Task\n",
    "\n",
    "One way that we can have the LLM do filtering is by extracting the abstract and maybe methodology sections through PubMed's BioC API.\n",
    "Therefore we don't have to worry about going through the downloaded articles, unzip their parent folders and parse through the PDF, we could just get the Abstract and Methodolgies returned to us in plain text format and feed it into the LLM.\n",
    "\n",
    "I'm able to get those fields returned from the API for several articles so this seems promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPa2XvpqpOWK"
   },
   "source": [
    "# Optimizing Search Queries\n",
    "**All the queries we're making are getting translated by PubMed into a different string that tries to capture our original query string that is then used to query the database.** Because of this papers that might not be relevant to the researchers is being returned.\n",
    "\n",
    "### Solutions?\n",
    "**Is there any way we can have more control over the translated string?**\n",
    "\n",
    "\n",
    "- Using MeSH Headings in the Query | [A description of the MeSH hierarchy](https://www.nlm.nih.gov/mesh/intro_trees.html)\n",
    "\n",
    "  - Automatic Explosion causes MeSH terms that are not included in the query, both original and translated, to be included in the search results. This can broaden the returned papers which might not be relevant. So a solution to this is to either select MeSH terms that when exploded still included terms that are relevant to us or by writing our query with terms that won't cause explosion, *i.e.* leaf terms that don't have any children- refer to picture below for an example.\n",
    "\n",
    "- Using MeSH **subheadings** in the Query\n",
    "\n",
    "  - Subheadings act as qualifiers for MeSH Headings [List of MeSH qualifiers](https://www.nlm.nih.gov/mesh/subhierarchy.html)\n",
    "  - Like MeSH headings, subheadings also explode\n",
    "\n",
    "- Limiting the use of Automatic Term Mapping?\n",
    "  - PubMed's search engine maps entry terms, **i.e.** terms with the field [\"All Fields\"], to a common MeSH heading depending on their similarity.\n",
    "  - We can limit this by using **Search Field** tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL3AR6p6igX8"
   },
   "source": [
    "#Recording Relevant MeSH Headings\n",
    "\n",
    "- Chemicals and Drugs Category\n",
    "  - Macromolecular Substances\n",
    "  - Amino Acids, Peptides, and Proteins\n",
    "  - Biological Factors\n",
    "    - Biomarkers\n",
    "    - Blood Coagulation Factor Inhibitors\n",
    "    - Blood Coagulation Factors\n",
    "- Diseases Category\n",
    "  - Nervous System Diseases\n",
    "- Anatomy Category\n",
    "  - Nervous System\n",
    "  - Cells\n",
    "- Psychiatry and Psychology Category\n",
    "  - Psychological Phenomena\n",
    "  - Mental Disroders\n",
    "- Pharmacological Actions Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s4R-AmQ7_4C"
   },
   "source": [
    "# Ideas\n",
    "\n",
    "- How crazy would it be to have the LLM generate the query string itslef?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering papers using Ollama at Oceanus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in ./adbmo_venv/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./adbmo_venv/lib/python3.10/site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: certifi in ./adbmo_venv/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: idna in ./adbmo_venv/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in ./adbmo_venv/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: anyio in ./adbmo_venv/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in ./adbmo_venv/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./adbmo_venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in ./adbmo_venv/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./adbmo_venv/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n",
      "Requirement already satisfied: tiktoken in ./adbmo_venv/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./adbmo_venv/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./adbmo_venv/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./adbmo_venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./adbmo_venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./adbmo_venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./adbmo_venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs4JSjVk-jcN"
   },
   "source": [
    "## Using BioC API to return full articles from PMC in JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66OugbJeGEM7"
   },
   "source": [
    "# Experimenting with Ollama for filtering\n",
    "\n",
    "* Design prompt for filtering\n",
    "* Pick Models\n",
    "* Test classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving Papers\n",
    "* Extract title or article using OCR or just type it in\n",
    "* Request BioC API to retreive full article from Pubmed Central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "article_info = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open(\"valid.txt\", 'r') as f1:\n",
    "    with open('valid_articles.txt', 'r') as f2:\n",
    "        for id, line in zip(f1, f2):\n",
    "            article_info[id[:-1]] = {'class': 1, \"title\": line[:-1]}\n",
    "        f2.close()\n",
    "    f1.close()\n",
    "with open(\"invalid.txt\", 'r') as f1:\n",
    "    with open('invalid_articles.txt', 'r') as f2:\n",
    "        for id, line in zip(f1, f2):\n",
    "            article_info[id[:-1]] = {'class': 0, \"title\": line[:-1]}\n",
    "        f2.close()\n",
    "    f1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.parse import quote\n",
    "# def search_article_ids(query, max_articles_per_query, api_key = \"5209f5d2ecb2ce377d8c20c5b5a08fd46f09\"):\n",
    "#     \"\"\"\n",
    "#     Search articles matching the query, return their ID\n",
    "#     \"\"\"\n",
    "#     search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "#     all_article_ids = []\n",
    "\n",
    "\n",
    "#     for start in range(0, max_articles_per_query):\n",
    "#         params = {\n",
    "#             \"db\": \"pmc\",\n",
    "#             \"term\": f'{query}',\n",
    "#             \"retmode\": \"json\",\n",
    "#             \"api_key\": api_key,\n",
    "#             \"retstart\": start,\n",
    "#             \"retmax\": 1\n",
    "#         }\n",
    "#         response = requests.get(search_url, params=params)\n",
    "#         try:\n",
    "#           if response.status_code == 200:\n",
    "#               data = response.json()\n",
    "#               article_ids = data[\"esearchresult\"][\"idlist\"]\n",
    "#               if not article_ids:\n",
    "#                   break  # No more articles found\n",
    "#               all_article_ids.extend(article_ids)\n",
    "#               time.sleep(.5)  # To respect PubMed's rate limit\n",
    "#           else:\n",
    "#             # While response status code is 500 then increase wait time and retry\n",
    "#               print(f\"Failed to search article IDs at {start}: {response.status_code}\\n{query}\\nTrying again\")\n",
    "#               time.sleep(3)\n",
    "#               while response.status_code == 500:\n",
    "#                 params = {\n",
    "#                   \"db\": \"pmc\",\n",
    "#                   \"term\": query,\n",
    "#                   \"retmode\": \"json\",\n",
    "#                   \"api_key\": api_key,\n",
    "#                   \"retstart\": start,\n",
    "#                   \"retmax\": 100\n",
    "#                 }\n",
    "#                 response = requests.get(search_url, params=params)\n",
    "#                 if response.status_code == 200:\n",
    "#                   data = response.json()\n",
    "#                   article_ids = data[\"esearchresult\"][\"idlist\"]\n",
    "#                 if not article_ids:\n",
    "#                   print(f\"No more articles found at {start}\")\n",
    "#                   break  # No more articles found\n",
    "#                 all_article_ids.extend(article_ids)\n",
    "#           all_article_ids.extend(article_ids)\n",
    "#         except KeyError as e:\n",
    "#           print(f\"Key Error found at {start}\")\n",
    "#           continue\n",
    "#         except json.decoder.JSONDecodeError as e:\n",
    "#           print(f\"JSON Decode Error found at {start}\")\n",
    "#           continue\n",
    "#               # break\n",
    "#     return all_article_ids\n",
    "\n",
    "# for title in titles:\n",
    "#     title['pmc_id'] = search_article_ids(title['title'], 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ids = []\n",
    "invalid_ids = []\n",
    "\n",
    "with open('valid.txt', 'r') as f:\n",
    "    valid_ids = [line.strip() for line in f]\n",
    "\n",
    "with open('invalid.txt', 'r') as f:\n",
    "    invalid_ids = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_text(article_id):\n",
    "    response = requests.get(f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/{article_id}/unicode\")\n",
    "\n",
    "    summary = {\n",
    "        'Abstract': [],\n",
    "        'Method': []\n",
    "    }\n",
    "\n",
    "    if response.text == f'No record can be found for the input: pmc{article_id[3:]}':\n",
    "        print(f\"No record found for article {article_id}.\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # print(len(response.text[1:-1]))\n",
    "            data = json.loads(response.text[1:-1])\n",
    "            for text in data['documents'][0]['passages'][:]:\n",
    "                if text['infons']['section_type'] == 'ABSTRACT':\n",
    "                    summary['Abstract'].append(text['text'])\n",
    "                if text['infons']['section_type'] == 'METHODS':\n",
    "                    summary['Method'].append(text['text'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            data = json.loads(response.text[e.colno+2:-1])\n",
    "            for text in data['documents'][0]['passages'][:]:\n",
    "                if text['infons']['section_type'] == 'ABSTRACT':\n",
    "                    summary['Abstract'].append(text['text'])\n",
    "                if text['infons']['section_type'] == 'METHODS':\n",
    "                    summary['Method'].append(text['text'])\n",
    "\n",
    "    summary['Abstract'] = '\\n'.join(summary['Abstract'])\n",
    "    summary['Method'] = '\\n'.join(summary['Method'])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "No record found for article PMC10612412.\n",
      "\n",
      "No record found for article PMC11065015.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     article_info[\u001b[38;5;28mid\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_full_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(article_info[\u001b[38;5;28mid\u001b[39m])\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mfetch_full_text\u001b[0;34m(article_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_full_text\u001b[39m(article_id):\n\u001b[0;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marticle_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/unicode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     summary \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m      7\u001b[0m     }\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo record can be found for the input: pmc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_id[\u001b[38;5;241m3\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Desktop/College/Research/ADBMO-UNLV/adbmo_venv/lib/python3.10/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for id in valid_ids:\n",
    "    try:\n",
    "        article_info[id]['summary'] = fetch_full_text(id)\n",
    "    except TypeError:\n",
    "        print(article_info[id])\n",
    "    \n",
    "for id in invalid_ids:\n",
    "    if id == '---' :\n",
    "        continue\n",
    "    try:\n",
    "        article_info[id]['summary'] = fetch_full_text(id)\n",
    "    except TypeError:\n",
    "        print(article_info[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_info['PMC10047620'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "error",
     "timestamp": 1731565457827,
     "user": {
      "displayName": "Abreham Tadesse",
      "userId": "00073469144777826962"
     },
     "user_tz": 480
    },
    "id": "IrAV1eH4J5r6",
    "outputId": "442b75e2-b57d-4f6f-d125-2ef3b76c3517"
   },
   "outputs": [],
   "source": [
    "list_models_path = \"http://oceanus.cs.unlv.edu:11434/api/tags\"\n",
    "res = requests.get(list_models_path)\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_template = \"\"\"\n",
    "You are going to be performing classifications on research articles regarding Alzheimer’s disease. Below are the rules on how \n",
    "to judge an article as being relevant or not. Please return a single Yes or No in your response.\n",
    "\n",
    "## 1. Papers Must Be Original Research Articles\n",
    "    * Metadata Filtering: Use metadata to identify papers labeled \"original research\" and exclude reviews, perspectives, posters, or preprints.\n",
    "    * Keyword Identification: Scan sections for phrases like \"data were collected\" or \"we conducted\" to confirm original research.\n",
    "## 2. Papers must have an AD focus, including a population of AD patients (at risk/MCI/AD) and/or looking at Alzheimer’s disease specific biomarkers (amyloid) (Many papers investigating neurodegenerative diseases will mention Alzheimer’s, even if it is not the focus)  \n",
    "    * Criteria for Selection:\n",
    "        * AD-Focused: Include papers explicitly studying Alzheimer’s disease (AD) topics like diagnosis, treatment, biomarkers (e.g., amyloid), or pathology.\n",
    "        * AD Patients: Papers involving AD populations (at risk, MCI, or diagnosed) are relevant, even if AD is not the central focus.\n",
    "        * Subset Context: Unless biomarkers or pathology are specific to AD, exclude papers focusing broadly on neurodegeneration with AD patients.\n",
    "        * Biomarker Specificity: Include studies addressing AD biomarkers (e.g., amyloid, tau). Exclude them if they contain general neurodegeneration markers.\n",
    "## 3.  Human Sample Size Must Be Over 50\n",
    "    * Criteria for Inclusion:\n",
    "        * Stated Sample Size: Include papers explicitly reporting a sample size of 50+ for AD patients (at risk, MCI, or diagnosed).\n",
    "        * Missing Information: Exclude papers without specific sample size details unless other critical criteria (e.g., strong AD focus or biomarker analysis) are met.\n",
    "## 4.  Must be looking at a protein (no genes, transcripts, or fragments)\n",
    "    * Keyword Filtering: Use terms like \"protein,\" \"amyloid,\" \"tau,\" or specific AD-related proteins (e.g., \"beta-amyloid\") to identify relevant studies. Exclude papers mentioning \"gene,\" \"RNA,\" \"transcription,\" or \"fragment\" as indicators of a non-protein focus.\n",
    "## 5. Include Fluids from Non-Clinical Models (Exclude Tissue Samples)\n",
    "    * Fluid Criteria: Focus on animal studies using fluids like cerebrospinal fluid (CSF), blood, serum, or plasma. These fluids often contain biomarkers relevant to AD research.\n",
    "    * Exclusion of Tissue Samples: Exclude studies involving tissue samples (e.g., brain slices, biopsy samples) using keywords like \"tissue,\" \"histology,\" or \"brain slice.\"\n",
    "## 6. Exclude “Blood Pressure” When Analyzing “Blood”\n",
    "    * Keyword Exclusion: Identify \"blood\" as a relevant biomarker but exclude papers mentioning \"blood pressure\" (e.g., \"blood pressure measurement\" or \"high blood pressure\").\n",
    "    * Contextual Filtering: Differentiate between \"blood\" used in biomarker sampling (e.g., \"serum analysis\") and circulatory assessments like \"blood pressure.\"\n",
    "    * Pattern Recognition: Exclude studies such as \"hypertension study\" or \"vascular health,\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"\"\"\n",
    "Here's a summary of an article that I want you to classify. Please respond with a yes or a no.\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "Title\n",
    "{article_info[\"PMC10047620\"]['title']}\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "Abstract\n",
    "{article_info[\"PMC10047620\"]['summary']['Abstract']}\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "Method\n",
    "{article_info[\"PMC10047620\"]['summary']['Method']}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = f\"\"\"\n",
    "Here's a summary of an article that I want you to classify. Please respond with a yes or a no.\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "Title\n",
    "{article_info[\"PMC7649343\"]['title']}\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "Abstract\n",
    "{article_info[\"PMC7649343\"]['summary']['Abstract']}\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "Method\n",
    "{article_info[\"PMC7649343\"]['summary']['Method']}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_info[\"PMC10047620\"]['summary']['Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_art.txt', 'w') as f:\n",
    "    f.write(article_info[\"PMC7649343\"]['title'])\n",
    "    f.write('\\n-------------------\\n')\n",
    "    f.write('\\n'.join(article_info[\"PMC7649343\"]['summary']['Abstract']))\n",
    "    f.write('\\n-------------------\\n')\n",
    "    f.write('\\n'.join(article_info[\"PMC7649343\"]['summary']['Method']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "print(len(tokenizer.encode(custom_template)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4215\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.encode(prompt_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1731565536708,
     "user": {
      "displayName": "Abreham Tadesse",
      "userId": "00073469144777826962"
     },
     "user_tz": 480
    },
    "id": "XLAfRoXyGVut"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "generate_path = \"http://oceanus.cs.unlv.edu:11434/api/generate\"\n",
    "# generate_path = \"http://127.0.0.1:11434/api/generate\"\n",
    "models = [\"custom-llama3.2:latest\",\"llama3.2:1b\", \"llama3.2:3b\",\"phi3.5:3.8b\", \"llama3.1:70b\", \"medllama2:7b\", \"mistral:7b\"]\n",
    "\n",
    "params = {\n",
    "    \"model\": models[3],\n",
    "    # \"system\": custom_template,\n",
    "    \"prompt\": question + custom_template ,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "generate_response = requests.post(generate_path, json=params)\n",
    "generate_response = generate_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "print(generate_response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_model = \"http://oceanus.cs.unlv.edu:11434/api/show\"\n",
    "show_model = \"http://127.0.0.1:11434/api/show\"\n",
    "params = {\n",
    "    # \"name\": \"llama3.2:1b\",\n",
    "    # \"name\": \"phi3.5:3.8b\"\n",
    "    \"name\": \"custom-llama3.2:latest\"\n",
    "}\n",
    "\n",
    "res = requests.post(show_model, json=params)\n",
    "model_info = res.json()\n",
    "# model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_info['template'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_template' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 8\u001b[0m         { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcustom_template\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      9\u001b[0m         { \n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         }\n\u001b[1;32m     13\u001b[0m     ] \n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_template' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI \n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = [\n",
    "        { \"role\": 'system', \"content\": f'{custom_template}'},\n",
    "        { \n",
    "            \"role\": 'user',\n",
    "            \"content\": f\"{question}\"\n",
    "        }\n",
    "    ] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AVqexlVLYzeGOSxujkXNcaGvctjLG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='No', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732153955, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=1, prompt_tokens=3541, total_tokens=3542, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_res(model_res):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import requests\n",
    "    import numpy as np\n",
    "\n",
    "    embedding_path = \"http://oceanus.cs.unlv.edu:11434/api/embeddings\"\n",
    "\n",
    "    params = {\n",
    "        'model': \"mxbai-embed-large\",\n",
    "        'prompt': f\"{model_res}\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(embedding_path, json=params)\n",
    "\n",
    "    params = {\n",
    "        'model': \"mxbai-embed-large\",\n",
    "        # 'prompt': \"The queen of england loves conquering africa\"\n",
    "        'prompt': \"Yes\"\n",
    "    }\n",
    "\n",
    "    response2 = requests.post(embedding_path, json=params)\n",
    "\n",
    "    emb1 = np.array(response.json()['embedding'])\n",
    "    emb2 = np.array(response2.json()['embedding'])\n",
    "\n",
    "    # emb1 = emb1.reshape(-1,1)\n",
    "    # emb2 = emb2.reshape(-1,1)\n",
    "\n",
    "    \n",
    "\n",
    "    similarity = cosine_similarity([emb1], [emb2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "0.6627921060540759\n",
      "-------------------------------------\n",
      "---\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n",
      "0.46856598127534244\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./classification_results/classification_results_llama3.2:1b.json', 'r') as f:\n",
    "    articles_classified = json.loads(f.read())\n",
    "\n",
    "for article in articles_classified:\n",
    "    try:\n",
    "        # print(articles_classified[article][\"ollama_model_response\"])\n",
    "        # print(articles_classified[article][\"openai_response\"])\n",
    "        # print(articles_classified[article]['class'])\n",
    "        print('-------------------------------------')\n",
    "        # similarity_score = validate_res(articles_classified[article][\"ollama_model_response\"])\n",
    "        similarity_score = validate_res(articles_classified[article][\"openai_response\"])\n",
    "        print(similarity_score)\n",
    "    except KeyError:\n",
    "        print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, I would classify this article as:\n",
      "\n",
      "**Yes**\n",
      "\n",
      "Reasoning:\n",
      "* The article appears to be an original research article, as indicated by the use of phrases like \"we analyzed.\"\n",
      "* The focus is clearly on Alzheimer's disease (AD), with specific attention to AD biomarkers and pathology.\n",
      "* Human sample sizes are not explicitly mentioned in the provided text; however, given the context, it seems reasonable to assume that human samples were used, even if the exact number isn't stated. In this case, I'll err on the side of caution and consider other factors more heavily.\n",
      "* The study involves analyzing proteins (NfL) rather than genes or transcripts.\n",
      "* Fluids from both clinical (human) and non-clinical models (5×FAD mice) are analyzed, which fits within the criteria.\n",
      "* \"Blood\" is used in a biomarker context (\"serum analysis\") rather than referring to blood pressure.\n",
      "Yes\n",
      "-------------------------------------\n",
      "Please provide the text of a research article regarding Alzheimer's disease. I will evaluate it based on the rules you provided and return a single answer: Yes (the article is relevant) or No (the article is not relevant).\n",
      "No\n",
      "-------------------------------------\n",
      "Based on the provided text, I will judge whether it meets the criteria for relevance. \n",
      "\n",
      "YES\n",
      "No\n",
      "-------------------------------------\n",
      "Based on the provided text, I would classify this article as:\n",
      "\n",
      "**Yes**\n",
      "\n",
      "Reasoning:\n",
      "- The article appears to be an original research article (Rule 1), discussing the results of a clinical trial for Alzheimer's disease treatment.\n",
      "- It has a strong focus on Alzheimer's disease, including the population at risk and biomarkers specific to AD (amyloid) (Rule 2).\n",
      "- The sample size is not explicitly mentioned in the provided text, but it discusses \"participants\" without specifying a number below 50, which may be assumed to meet Rule 3.\n",
      "- It looks at proteins relevant to Alzheimer's disease, specifically mentioning amyloid and tau (Rule 4).\n",
      "- Although not exclusively focused on fluids from non-clinical models, there is mention of plasma biomarkers, which might align with Rule 5. However, the primary focus seems to be on human clinical trials rather than animal studies.\n",
      "- The article does not appear to analyze blood pressure in relation to blood; instead, it discusses the use of plasma for biomarker analysis (Rule 6).\n",
      "Yes\n",
      "-------------------------------------\n",
      "No\n",
      "Yes\n",
      "-------------------------------------\n",
      "Yes\n",
      "No\n",
      "-------------------------------------\n",
      "No\n",
      "Yes\n",
      "-------------------------------------\n",
      "I'm ready to help. Please provide the article you'd like me to classify, and I'll respond with a single \"Yes\" or \"No\" based on the provided rules.\n",
      "Yes\n",
      "-------------------------------------\n",
      "No. \n",
      "\n",
      "The article discusses the relationship between blood pressure variability (BPV), plasma neurofilament light chain (pNfL) levels, and brain structure/cognitive performance in older adults. While it involves human samples and focuses on a protein (pNfL), it does not meet the criteria for exclusion of \"Blood Pressure\" when analyzing \"Blood\". The article actually investigates the impact of blood pressure variability on pNfL levels and cognitive outcomes, making it irrelevant to the specified rules.\n",
      "No\n",
      "-------------------------------------\n",
      "No\n",
      "Yes\n",
      "-------------------------------------\n",
      "Yes\n",
      "Yes\n",
      "-------------------------------------\n",
      "Yes.\n",
      "Yes\n",
      "-------------------------------------\n",
      "Based on the provided text, I will evaluate whether it meets the criteria for relevance to Alzheimer's disease research.\n",
      "\n",
      "1. The paper appears to be an original research article, describing two Phase 3 clinical trials and their results in a Japanese subgroup of patients with early Alzheimer's disease.\n",
      "\n",
      "2. The focus is clearly on Alzheimer's disease (AD), specifically on the efficacy and safety of aducanumab in treating AD in a Japanese population.\n",
      "\n",
      "3. The sample size for the Japanese subgroup is not explicitly stated, but it can be inferred that it exceeds 50 since the paper mentions \"a total of 3245 patients\" enrolled across both studies.\n",
      "\n",
      "4. While the text does mention biomarkers and amyloid, which are proteins relevant to AD research, it doesn't specifically focus on a protein or exclude genes, transcripts, or fragments as required by criterion 4.\n",
      "\n",
      "5. The study involves human subjects and discusses the analysis of aducanumab's effects in patients' brains through imaging techniques (amyloid PET), which might not strictly fit the criterion focused on fluids from non-clinical models but does involve analysis relevant to AD biomarkers.\n",
      "\n",
      "6. There is no mention of \"blood pressure\" or any context that would suggest exclusion based on this criterion.\n",
      "\n",
      "Given these considerations, I will classify the article as being generally relevant to Alzheimer's disease research but note some ambiguity regarding criteria 4 and 5.\n",
      "\n",
      "**Yes**\n",
      "Yes\n",
      "-------------------------------------\n",
      "Yes\n",
      "Yes\n",
      "-------------------------------------\n",
      "Please provide the article text, and I will judge its relevance based on the provided rules. I'll respond with either \"Yes\" or \"No\".\n",
      "Yes\n",
      "-------------------------------------\n",
      "---\n",
      "I'm ready to help with classifying research articles regarding Alzheimer's disease. Please provide the article text, and I'll follow the rules you provided to return a single Yes or No in my response indicating whether the article is relevant or not.\n",
      "No\n",
      "-------------------------------------\n",
      "Yes\n",
      "No\n",
      "-------------------------------------\n",
      "No\n",
      "No\n",
      "-------------------------------------\n",
      "Yes.\n",
      "No\n",
      "-------------------------------------\n",
      "No\n",
      "No\n",
      "-------------------------------------\n",
      "Yes.\n",
      "No\n",
      "-------------------------------------\n",
      "Please provide the text of a research article regarding Alzheimer's disease, and I will classify it according to the rules provided. Please paste the text, and I'll respond with a single \"Yes\" or \"No\".\n",
      "No\n",
      "-------------------------------------\n",
      "No.\n",
      "No\n",
      "-------------------------------------\n",
      "Please go ahead with the article summary. I'll respond with a Yes or No based on the rules provided.\n",
      "No\n",
      "-------------------------------------\n",
      "No\n",
      "No\n",
      "-------------------------------------\n",
      "No. \n",
      "\n",
      "This article does not focus on a protein, but rather explores the pathophysiology behind the relationship between cognitive impairment (CI) and heart attacks/heart disease, discussing risk factors for CI in patients with heart attacks, interventions for heart attacks, and medical management.\n",
      "No\n",
      "-------------------------------------\n",
      "Please provide the text of a research article related to Alzheimer's disease, and I will classify it according to the rules provided. Please go ahead and paste the article, and I'll respond with either \"Yes\" (indicating the article meets the criteria) or \"No\".\n",
      "No\n",
      "-------------------------------------\n",
      "I'm ready to classify the article. Please provide the summary, and I'll respond with a single \"Yes\" or \"No\".\n",
      "No\n",
      "-------------------------------------\n",
      "No\n",
      "Yes\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./classification_results/classification_results_llama3.1:70b.json', 'r') as f:\n",
    "    articles_classified = json.loads(f.read())\n",
    "\n",
    "for article in articles_classified:\n",
    "    try:\n",
    "        print(articles_classified[article][\"ollama_model_response\"])\n",
    "        print(articles_classified[article][\"openai_response\"])\n",
    "        print('-------------------------------------')\n",
    "    except KeyError:\n",
    "        print(article)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqPiESVqxtTdm9T0WVpQQP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adbmo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
