{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "class LLMInterface:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_auth_token = os.getenv('HF_ACCESS_TOKEN'))\n",
    "\n",
    "    def preprocesser_inference(\n",
    "            self,\n",
    "            input: list[str],\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        tokenized_text = []\n",
    "        for text in input:\n",
    "            tk_txt = self.tokenizer(text, max_length=max_length,padding=pad, truncation=truncate, return_tensors=return_tensor).to(self.device)\n",
    "            tokenized_text.append(tk_txt)\n",
    "        return tokenized_text\n",
    "    \n",
    "class classifierInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, id2label: dict, label2id: dict, num_labels=2, device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        load_dotenv()\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda:0\" else torch.float32,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "\n",
    "    def inference_classify(\n",
    "            self,\n",
    "            inputs: list[str],\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        classes = []\n",
    "        inputs = self.preprocesser_inference(inputs, pad, truncate, max_length, return_tensor)\n",
    "        for input in inputs:\n",
    "            with torch.no_grad():\n",
    "                input = {key: value.to(self.device) for key, value in input.items()}\n",
    "                logits = self.model(**input).logits\n",
    "            predicted_class = logits.argmax().item()\n",
    "            classes.append(self.model.config.id2label[predicted_class])\n",
    "        return classes\n",
    "    \n",
    "class extractorInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import AutoModelForTokenClassification\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def inference_extract(\n",
    "            self,\n",
    "            input: str,\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: bool,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        input = self.preprocesser_inference(input, pad, truncate, max_length, return_tensor)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**input).logits\n",
    "        \n",
    "        # TODO: RETURN THE APPROPRIATE VALUE, NOT LOGITS, FROM HERE\n",
    "\n",
    "class EncoderDecoder_Interface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   \n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def preprocesser_inference(self, input, pad, truncate, max_length, return_tensor = \"pt\"):\n",
    "        return self.tokenizer(input, max_length=max_length,padding=pad, truncation=truncate, return_tensors=return_tensor).to(self.device)\n",
    "    \n",
    "    def inference_seq(\n",
    "            self,\n",
    "            input: str,\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        input = self.preprocesser_inference(input, pad, truncate, max_length, return_tensor)\n",
    "        # input = {key: value.to(self.device) for key, value in input.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **input,\n",
    "                max_length=50,\n",
    "                num_beams=4,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return output_text\n",
    "\n",
    "class EncoderOnly_EDInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import T5EncoderModel, T5Tokenizer\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5EncoderModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "\n",
    "    def inference_classify_enc(\n",
    "            self,\n",
    "            inputs: list[str],\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        classes = []\n",
    "        inputs = self.preprocesser_inference(inputs, pad, truncate, max_length, return_tensor)\n",
    "        for input in inputs:\n",
    "            with torch.no_grad():\n",
    "                input = {key: value.to(self.device) for key, value in input.items()}\n",
    "                logits = self.model(**input)\n",
    "            predicted_class = logits[0]\n",
    "            classes.append(self.model.config.id2label[predicted_class])\n",
    "        return classes\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open('/home/tadesa1/ADBMO-UNLV/SentrySys_Experiments/classification_results_openai.json', 'r') as f:\n",
    "    classified_results = json.load(f)\n",
    "\n",
    "del classified_results['---']\n",
    "\n",
    "def data_generator():\n",
    "    for obj in classified_results:\n",
    "        try:\n",
    "            yield {\"pmid\": obj, \"true_class\": classified_results[obj]['class'], \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "        except KeyError:\n",
    "            yield {\"pmid\": obj, \"true_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1, \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "\n",
    "data = Dataset.from_generator(data_generator)\n",
    "false_class_data = data.filter(lambda e: e['true_class'] == 0)\n",
    "true_class_data = data.filter(lambda e: e['true_class'] == 1)\n",
    "\n",
    "test_data = []\n",
    "for paper in data:\n",
    "    test_data.append([paper['title']+paper['abstract']+paper['method']])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model_names = [\"bert-base-uncased\", \"distilbert-base-uncased-finetuned-sst-2-english\",\"nlptown/bert-base-multilingual-uncased-sentiment\"]\n",
    "    \n",
    "    id2label = {\n",
    "        0: 'IRRELEVANT',\n",
    "        1: 'RELEVANT'        \n",
    "    }\n",
    "\n",
    "    label2id = {\n",
    "        'IRRELEVANT': 0,\n",
    "        'RELEVANT': 1        \n",
    "    }\n",
    "    classifier = classifierInterface(model_name=model_names[0], id2label=id2label, label2id=label2id)\n",
    "\n",
    "    result = []\n",
    "    for paper in data:\n",
    "        test_data = [paper['title']+paper['abstract']+paper['method']]\n",
    "        result.append(classifier.inference_classify(inputs=test_data, pad = True, truncate = True, max_length=512))\n",
    "        # print(f\"{result} : {paper['predicted_class']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for encoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "true_labels = [example['true_class'] for example in data]\n",
    "# predicted_labels = [print(example[0]) for example in result]\n",
    "predicted_labels = [1 if res[0] == 'RELEVANT' else 0 for res in result]\n",
    "\n",
    "\n",
    "print(type(true_labels))\n",
    "print(type(predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    " \n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder-Decoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez un bon soir!\n"
     ]
    }
   ],
   "source": [
    "models = [\"google/flan-t5-large\"]\n",
    "ed_model = EncoderDecoder_Interface(models[0])\n",
    "\n",
    "input_txt = \"translate English to French: Have a good night!\"\n",
    "# input_txt = \"Q: What is 8 raised to the power of 1/3? Give the rationale before answering.\"\n",
    "output_txt = ed_model.inference_seq(input = input_txt, pad=False, truncate=False, max_length=512)\n",
    "print(output_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder only ED models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'id2label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 35\u001b[0m\n\u001b[1;32m     29\u001b[0m label2id \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIRRELEVANT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRELEVANT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m        \n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m encoder_ed_model \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderOnly_EDInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'id2label'"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from datasets import Dataset\n",
    "\n",
    "with open('/home/tadesa1/ADBMO-UNLV/SentrySys_Experiments/classification_results_openai.json', 'r') as f:\n",
    "    classified_results = json.load(f)\n",
    "\n",
    "del classified_results['---']\n",
    "\n",
    "def data_generator():\n",
    "    for obj in classified_results:\n",
    "        try:\n",
    "            yield {\"pmid\": obj, \"true_class\": classified_results[obj]['class'], \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "        except KeyError:\n",
    "            yield {\"pmid\": obj, \"true_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1, \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "\n",
    "data = Dataset.from_generator(data_generator)\n",
    "false_class_data = data.filter(lambda e: e['true_class'] == 0)\n",
    "true_class_data = data.filter(lambda e: e['true_class'] == 1)\n",
    "\n",
    "test_data = []\n",
    "for paper in data:\n",
    "    test_data.append([paper['title']+paper['abstract']+paper['method']])\n",
    "\n",
    "id2label = {\n",
    "    0: 'IRRELEVANT',\n",
    "    1: 'RELEVANT'        \n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    'IRRELEVANT': 0,\n",
    "    'RELEVANT': 1        \n",
    "}\n",
    "\n",
    "models = [\"google/flan-t5-large\"]\n",
    "encoder_ed_model = EncoderOnly_EDInterface(models[0], id2label=id2label, label2id=label2id)\n",
    "\n",
    "result = []\n",
    "for paper in data:\n",
    "    test_data = [paper['title']+paper['abstract']+paper['method']]\n",
    "    result.append(encoder_ed_model.inference_classify_enc(inputs=test_data, pad = True, truncate = True, max_length=512))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "true_labels = [example['true_class'] for example in data]\n",
    "# predicted_labels = [print(example[0]) for example in result]\n",
    "predicted_labels = [1 if res[0] == 'RELEVANT' else 0 for res in result]\n",
    "\n",
    "\n",
    "print(type(true_labels))\n",
    "print(type(predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    " \n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset design\n",
    "\n",
    "#### How will your datasets be setup?\n",
    "\n",
    "* For enc-dec models you'd benefit from instruction fine tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "* Will performance improve in all aspects if we perform unsupervised fine tuning on the models?\n",
    "\n",
    "The goal is to have a general purpose ADBMO model that we can recycle for different purposes. A chatbot with reasoning capabilities, text extraction and classification, and even document searching abilities.\n",
    "- Let's start off by training adapters that we can plugin to existing models. Let's measure the performance of that for our downstream tasks like text extraction and classification and then see if further fine-tuning is necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
