{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "class LLMInterface:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_auth_token = os.getenv('HF_ACCESS_TOKEN'))\n",
    "\n",
    "    def preprocesser_inference(\n",
    "            self,\n",
    "            input: list[str],\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        tokenized_text = []\n",
    "        for text in input:\n",
    "            tk_txt = self.tokenizer(text, max_length=max_length,padding=pad, truncation=truncate, return_tensors=return_tensor).to(self.device)\n",
    "            tokenized_text.append(tk_txt)\n",
    "        return tokenized_text\n",
    "    \n",
    "class classifierInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, id2label: dict, label2id: dict, num_labels=2, device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        load_dotenv()\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda:0\" else torch.float32,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "\n",
    "    def inference_classify(\n",
    "            self,\n",
    "            inputs: list[str],\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        classes = []\n",
    "        inputs = self.preprocesser_inference(inputs, pad, truncate, max_length, return_tensor)\n",
    "        for input in inputs:\n",
    "            with torch.no_grad():\n",
    "                input = {key: value.to(self.device) for key, value in input.items()}\n",
    "                logits = self.model(**input).logits\n",
    "            predicted_class = logits.argmax().item()\n",
    "            classes.append(self.model.config.id2label[predicted_class])\n",
    "        return classes\n",
    "    \n",
    "class extractorInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import AutoModelForTokenClassification\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def inference_extract(\n",
    "            self,\n",
    "            input: str,\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: bool,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        input = self.preprocesser_inference(input, pad, truncate, max_length, return_tensor)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**input).logits\n",
    "        \n",
    "        # TODO: RETURN THE APPROPRIATE VALUE, NOT LOGITS, FROM HERE\n",
    "\n",
    "class EncoderDecoder_Interface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   \n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def preprocesser_inference(self, input, pad, truncate, max_length, return_tensor = \"pt\"):\n",
    "        return self.tokenizer(input, max_length=max_length,padding=pad, truncation=truncate, return_tensors=return_tensor).to(self.device)\n",
    "    \n",
    "    def inference_seq(\n",
    "            self,\n",
    "            input: str,\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        input = self.preprocesser_inference(input, pad, truncate, max_length, return_tensor)\n",
    "        # input = {key: value.to(self.device) for key, value in input.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **input,\n",
    "                max_length=50,\n",
    "                num_beams=4,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return output_text\n",
    "\n",
    "class EncoderOnly_EDInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import T5EncoderModel, T5Tokenizer\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5EncoderModel.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "\n",
    "    def inference_classify_enc(\n",
    "            self,\n",
    "            inputs: list[str],\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        classes = []\n",
    "        inputs = self.preprocesser_inference(inputs, pad, truncate, max_length, return_tensor)\n",
    "        for input in inputs:\n",
    "            with torch.no_grad():\n",
    "                input = {key: value.to(self.device) for key, value in input.items()}\n",
    "                logits = self.model(**input)\n",
    "            predicted_class = logits[0]\n",
    "            classes.append(self.model.config.id2label[predicted_class])\n",
    "        return classes\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 02:46:35.471942: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 02:46:35.485102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743475595.499378 1155897 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743475595.504024 1155897 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 02:46:35.520433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocesser_inference() takes from 4 to 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     41\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m [paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 42\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_classify\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# print(f\"{result} : {paper['predicted_class']}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mclassifierInterface.inference_classify\u001b[0;34m(self, inputs, pad, truncate, max_length, return_tensor)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minference_classify\u001b[39m(\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m         inputs: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         return_tensor: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m ):\n\u001b[1;32m     51\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocesser_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mTypeError\u001b[0m: preprocesser_inference() takes from 4 to 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open('/home/tadesa1/ADBMO-UNLV/SentrySys_Experiments/classification_results_openai.json', 'r') as f:\n",
    "    classified_results = json.load(f)\n",
    "\n",
    "del classified_results['---']\n",
    "\n",
    "def data_generator():\n",
    "    for obj in classified_results:\n",
    "        try:\n",
    "            yield {\"pmid\": obj, \"true_class\": classified_results[obj]['class'], \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "        except KeyError:\n",
    "            yield {\"pmid\": obj, \"true_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1, \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "\n",
    "data = Dataset.from_generator(data_generator)\n",
    "false_class_data = data.filter(lambda e: e['true_class'] == 0)\n",
    "true_class_data = data.filter(lambda e: e['true_class'] == 1)\n",
    "\n",
    "test_data = []\n",
    "for paper in data:\n",
    "    test_data.append([paper['title']+paper['abstract']+paper['method']])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model_names = [\"bert-base-uncased\", \"distilbert-base-uncased-finetuned-sst-2-english\",\"nlptown/bert-base-multilingual-uncased-sentiment\"]\n",
    "    \n",
    "    id2label = {\n",
    "        0: 'IRRELEVANT',\n",
    "        1: 'RELEVANT'        \n",
    "    }\n",
    "\n",
    "    label2id = {\n",
    "        'IRRELEVANT': 0,\n",
    "        'RELEVANT': 1        \n",
    "    }\n",
    "    classifier = classifierInterface(model_name=model_names[0], id2label=id2label, label2id=label2id)\n",
    "\n",
    "    result = []\n",
    "    for paper in data:\n",
    "        test_data = [paper['title']+paper['abstract']+paper['method']]\n",
    "        result.append(classifier.inference_classify(inputs=test_data, pad = True, truncate = True, max_length=512))\n",
    "        # print(f\"{result} : {paper['predicted_class']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for encoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "true_labels = [example['true_class'] for example in data]\n",
    "# predicted_labels = [print(example[0]) for example in result]\n",
    "predicted_labels = [1 if res[0] == 'RELEVANT' else 0 for res in result]\n",
    "\n",
    "\n",
    "print(type(true_labels))\n",
    "print(type(predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    " \n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder-Decoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tadesa1/anaconda3/envs/nlp_ml_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/tadesa1/anaconda3/envs/nlp_ml_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alzheimer's, Brain Cancer\n"
     ]
    }
   ],
   "source": [
    "models = [\"google/flan-t5-large\", \"google-t5/t5-base\"]\n",
    "ed_model = EncoderDecoder_Interface(models[0])\n",
    "# ed_model = EncoderDecoder_Interface(models[1])\n",
    "\n",
    "\n",
    "input_txt = \"Extract the disease names mentioned in the following texts: Amyloid build up is usually associated with Alzheimer's. It can also be associated with other things like Brain Cancer\"\n",
    "# input_txt = \"What is not the protein in the following sentence but is still a marker: Amyloid build up is usually associated with Alzheimer's. It is accompanied with an increase in blood pressure\"\n",
    "# input_txt = \"Answer the following mathematical question. Give the rational before answering: What is the cube of 2?\"\n",
    "# input_txt = \"mnli premise: Donald Trump has a hair color that resemble the color generated by a hot object. hypothesis: Donald Trump has fire like hair\"\n",
    "output_txt = ed_model.inference_seq(input = input_txt, pad=False, truncate=False, max_length=512)\n",
    "print(output_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder only ED models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'id2label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 35\u001b[0m\n\u001b[1;32m     29\u001b[0m label2id \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIRRELEVANT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRELEVANT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m        \n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m encoder_ed_model \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderOnly_EDInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'id2label'"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from datasets import Dataset\n",
    "\n",
    "with open('/home/tadesa1/ADBMO-UNLV/SentrySys_Experiments/classification_results_openai.json', 'r') as f:\n",
    "    classified_results = json.load(f)\n",
    "\n",
    "del classified_results['---']\n",
    "\n",
    "def data_generator():\n",
    "    for obj in classified_results:\n",
    "        try:\n",
    "            yield {\"pmid\": obj, \"true_class\": classified_results[obj]['class'], \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "        except KeyError:\n",
    "            yield {\"pmid\": obj, \"true_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1, \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "\n",
    "data = Dataset.from_generator(data_generator)\n",
    "false_class_data = data.filter(lambda e: e['true_class'] == 0)\n",
    "true_class_data = data.filter(lambda e: e['true_class'] == 1)\n",
    "\n",
    "test_data = []\n",
    "for paper in data:\n",
    "    test_data.append([paper['title']+paper['abstract']+paper['method']])\n",
    "\n",
    "id2label = {\n",
    "    0: 'IRRELEVANT',\n",
    "    1: 'RELEVANT'        \n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    'IRRELEVANT': 0,\n",
    "    'RELEVANT': 1        \n",
    "}\n",
    "\n",
    "models = [\"google/flan-t5-large\"]\n",
    "encoder_ed_model = EncoderOnly_EDInterface(models[0], id2label=id2label, label2id=label2id)\n",
    "\n",
    "result = []\n",
    "for paper in data:\n",
    "    test_data = [paper['title']+paper['abstract']+paper['method']]\n",
    "    result.append(encoder_ed_model.inference_classify_enc(inputs=test_data, pad = True, truncate = True, max_length=512))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "true_labels = [example['true_class'] for example in data]\n",
    "# predicted_labels = [print(example[0]) for example in result]\n",
    "predicted_labels = [1 if res[0] == 'RELEVANT' else 0 for res in result]\n",
    "\n",
    "\n",
    "print(type(true_labels))\n",
    "print(type(predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    " \n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset design\n",
    "\n",
    "#### How will your datasets be setup?\n",
    "\n",
    "* For enc-dec models you'd benefit from instruction fine tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "* Will performance improve in all aspects if we perform unsupervised fine tuning on the models?\n",
    "\n",
    "The goal is to have a general purpose ADBMO model that we can recycle for different purposes. A chatbot with reasoning capabilities, text extraction and classification, and even document searching abilities.\n",
    "- Let's start off by training adapters that we can plugin to existing models. Let's measure the performance of that for our downstream tasks like text extraction and classification and then see if further fine-tuning is necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
