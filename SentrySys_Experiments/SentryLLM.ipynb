{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "class LLMInterface:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_auth_token = os.getenv('HF_ACCESS_TOKEN'))\n",
    "\n",
    "    def preprocesser_inference(\n",
    "            self,\n",
    "            input: list[str],\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        tokenized_text = []\n",
    "        for text in input:\n",
    "            tk_txt = self.tokenizer(text, max_length=max_length,padding=pad, truncation=truncate, return_tensors=return_tensor).to(self.device)\n",
    "            tokenized_text.append(tk_txt)\n",
    "        return tokenized_text\n",
    "    \n",
    "class classifierInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, id2label: dict, label2id: dict, num_labels=2, device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        load_dotenv()\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda:0\" else torch.float32,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "\n",
    "    def inference_classify(\n",
    "            self,\n",
    "            inputs: list[str],\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        classes = []\n",
    "        inputs = self.preprocesser_inference(inputs, pad, truncate, max_length, return_tensor)\n",
    "        for input in inputs:\n",
    "            with torch.no_grad():\n",
    "                input = {key: value.to(self.device) for key, value in input.items()}\n",
    "                logits = self.model(**input).logits\n",
    "            predicted_class = logits.argmax().item()\n",
    "            classes.append(self.model.config.id2label[predicted_class])\n",
    "        return classes\n",
    "    \n",
    "class extractorInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import AutoModelForTokenClassification\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def inference_extract(\n",
    "            self,\n",
    "            input: str,\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: bool,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        input = self.preprocesser_inference(input, pad, truncate, max_length, return_tensor)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**input).logits\n",
    "        \n",
    "        # TODO: RETURN THE APPROPRIATE VALUE, NOT LOGITS, FROM HERE\n",
    "\n",
    "class EncoderDecoder_Interface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM   \n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device==\"cuda\" else torch.float32,\n",
    "            token = os.getenv('HF_ACCESS_TOKEN')\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def preprocesser_inference(self, input, pad, truncate, max_length, return_tensor = \"pt\"):\n",
    "        return self.tokenizer(input, max_length=max_length,padding=pad, truncation=truncate, return_tensors=return_tensor).to(self.device)\n",
    "    \n",
    "    def inference_seq(\n",
    "            self,\n",
    "            input: str,\n",
    "            pad: bool,\n",
    "            truncate: bool,\n",
    "            max_length: int,\n",
    "            return_tensor: str = \"pt\"\n",
    "    ):\n",
    "        input = self.preprocesser_inference(input, pad, truncate, max_length, return_tensor)\n",
    "        input = {key: value.to(self.device) for key, value in input.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **input,\n",
    "                max_length=50,\n",
    "                num_beams=4,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return output_text\n",
    "\n",
    "class EncoderOnly_EDInterface(LLMInterface):\n",
    "    def __init__(self, model_name: str, device: str = 'cuda:0' if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__(model_name, device)\n",
    "        from transformers import T5EncoderModel, T5Tokenizer\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5EncoderModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open('/home/tadesa1/ADBMO-UNLV/SentrySys_Experiments/classification_results_openai.json', 'r') as f:\n",
    "    classified_results = json.load(f)\n",
    "\n",
    "del classified_results['---']\n",
    "\n",
    "def data_generator():\n",
    "    for obj in classified_results:\n",
    "        try:\n",
    "            yield {\"pmid\": obj, \"true_class\": classified_results[obj]['class'], \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "        except KeyError:\n",
    "            yield {\"pmid\": obj, \"true_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1, \"title\":classified_results[obj]['title'], \"abstract\": classified_results[obj]['summary']['Abstract'], \"method\": classified_results[obj]['summary']['Method'], \"predicted_class\": 0 if classified_results[obj]['openai_response'] == 'No' else 1}\n",
    "\n",
    "data = Dataset.from_generator(data_generator)\n",
    "false_class_data = data.filter(lambda e: e['true_class'] == 0)\n",
    "true_class_data = data.filter(lambda e: e['true_class'] == 1)\n",
    "\n",
    "test_data = []\n",
    "for paper in data:\n",
    "    test_data.append([paper['title']+paper['abstract']+paper['method']])\n",
    "\n",
    "result = []\n",
    "for paper in data:\n",
    "    test_data = [paper['title']+paper['abstract']+paper['method']]\n",
    "    result.append(classifier.inference_classify(inputs=test_data, pad = True, truncate = True, max_length=512))\n",
    "    # print(f\"{result} : {paper['predicted_class']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model_names = [\"bert-base-uncased\", \"distilbert-base-uncased-finetuned-sst-2-english\",\"nlptown/bert-base-multilingual-uncased-sentiment\"]\n",
    "    \n",
    "    id2label = {\n",
    "        0: 'IRRELEVANT',\n",
    "        1: 'RELEVANT'        \n",
    "    }\n",
    "\n",
    "    label2id = {\n",
    "        'IRRELEVANT': 0,\n",
    "        'RELEVANT': 1        \n",
    "    }\n",
    "    classifier = classifierInterface(model_name=model_names[0], id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "true_labels = [example['true_class'] for example in data]\n",
    "# predicted_labels = [print(example[0]) for example in result]\n",
    "predicted_labels = [1 if res[0] == 'RELEVANT' else 0 for res in result]\n",
    "\n",
    "\n",
    "print(type(true_labels))\n",
    "print(type(predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    " \n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Encoder-Decoder based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power of 1/3 is 8 * (1 / 3) = 4. The power of 4 is 8 * (1 / 3) = 4. The power of 8 is 4 * (1 / 3) = 8. So the final answer is 8.\n"
     ]
    }
   ],
   "source": [
    "models = [\"google/flan-t5-large\"]\n",
    "ed_model = EncoderDecoder_Interface(models[0])\n",
    "\n",
    "# input_txt = \"translate English to French: Have a good night!\"\n",
    "input_txt = \"Q: What is 8 raised to the power of 1/3? Give the rationale before answering.\"\n",
    "output_txt = ed_model.inference_seq(input = input_txt, pad=False, truncate=False, max_length=512)\n",
    "print(output_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "* Will performance improve in all aspects if we perform unsupervised fine tuning on the models?\n",
    "\n",
    "The goal is to have a general purpose ADBMO model that we can recycle for different purposes. A chatbot with reasoning capabilities, text extraction and classification, and even document searching abilities.\n",
    "- Let's start off by training adapters that we can plugin to existing models. Let's measure the performance of that for our downstream tasks like text extraction and classification and then see if further fine-tuning is necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
